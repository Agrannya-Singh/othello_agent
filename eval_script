{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":585569,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":437546,"modelId":454252}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This is a Kaggle notebook script to evaluate the trained Othello model ('othello_model_final.pth') \n# against a Minimax agent with depth 4. The notebook is optimized for background execution: \n# it uses tqdm for progress tracking, prints updates, and saves results to a file.\n# Assumptions:\n# - The model file 'othello_model_final.pth' is uploaded to the notebook (e.g., via Add Data > Your Files).\n# - If using GPU, enable it in Notebook Settings (Accelerator: GPU P100 or T4).\n# - The script runs sequentially but efficiently; Minimax depth 4 is fast (~10k nodes per move).\n# - Othello utils are included from training details.\n# - Evaluation: Play 100 games (50 with NN as Black, 50 as White), record win/draw/loss rates for the NN model.\n# - Output: Prints summary and saves detailed results to 'evaluation_results.txt'.\n\nimport math\nimport random\nimport copy\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nimport time\nimport os\n\n# Device setup (use GPU if available for NN inference)\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {DEVICE}\")\n\n# =============================================================================\n# OTHELLO GAME LOGIC (from othello_utils.py)\n# =============================================================================\nclass Othello:\n    def __init__(self, board=None, current_player=1):\n        if board is None:\n            self.board = np.zeros((8, 8), dtype=np.int8)\n            self.board[3, 3] = self.board[4, 4] = -1\n            self.board[3, 4] = self.board[4, 3] = 1\n        else:\n            self.board = board\n        self.current_player = current_player\n\n    def opponent(self, player: int) -> int: return -player\n\n    def get_legal_moves(self, player: int):\n        moves = []\n        for r in range(8):\n            for c in range(8):\n                if self.board[r,c] == 0 and self._is_valid_move(r, c, player):\n                    moves.append((r,c))\n        return moves\n\n    def _is_valid_move(self, row: int, col: int, player: int) -> bool:\n        if self.board[row, col] != 0: return False\n        for dr, dc in [(-1,-1),(-1,0),(-1,1),(0,-1),(0,1),(1,-1),(1,0),(1,1)]:\n            r, c = row + dr, col + dc\n            if 0 <= r < 8 and 0 <= c < 8 and self.board[r, c] == -player:\n                r, c = r + dr, c + dc\n                while 0 <= r < 8 and 0 <= c < 8:\n                    if self.board[r, c] == player: return True\n                    if self.board[r, c] == 0: break\n                    r, c = r + dr, c + dc\n        return False\n\n    def make_move(self, row: int, col: int, player: int):\n        self.board[row, col] = player\n        for dr, dc in [(-1,-1),(-1,0),(-1,1),(0,-1),(0,1),(1,-1),(1,0),(1,1)]:\n            flips = []\n            r, c = row + dr, col + dc\n            while 0 <= r < 8 and 0 <= c < 8 and self.board[r, c] == -player:\n                flips.append((r, c)); r, c = r + dr, c + dc\n            if 0 <= r < 8 and 0 <= c < 8 and self.board[r, c] == player:\n                for fr, fc in flips: self.board[fr, fc] = player\n        self.current_player = -player\n        return self\n\n    def is_game_over(self) -> bool: return not self.get_legal_moves(1) and not self.get_legal_moves(-1)\n    \n    def get_winner(self) -> int:\n        score = np.sum(self.board)\n        return 1 if score > 0 else -1 if score < 0 else 0\n\n# =============================================================================\n# NEURAL NETWORK MODEL (OthelloNetV3 from training)\n# =============================================================================\nclass ResidualBlock(nn.Module):\n    def __init__(self, num_channels):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(num_channels)\n        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(num_channels)\n    def forward(self, x):\n        identity = x; out = F.relu(self.bn1(self.conv1(x))); out = self.bn2(self.conv2(out)); out += identity; return F.relu(out)\n\nclass OthelloNetV3(nn.Module):\n    def __init__(self, num_res_blocks=8):\n        super(OthelloNetV3, self).__init__()\n        self.conv_in = nn.Conv2d(2, 128, kernel_size=3, padding=1); self.bn_in = nn.BatchNorm2d(128)\n        self.res_blocks = nn.Sequential(*[ResidualBlock(128) for _ in range(num_res_blocks)])\n        self.policy_conv = nn.Conv2d(128, 2, kernel_size=1); self.policy_bn = nn.BatchNorm2d(2); self.policy_fc = nn.Linear(2*8*8, 64)\n        self.value_conv = nn.Conv2d(128, 1, kernel_size=1); self.value_bn = nn.BatchNorm2d(1); self.value_fc1 = nn.Linear(1*8*8, 256); self.value_fc2 = nn.Linear(256, 1)\n\n    def forward(self, x):\n        x = F.relu(self.bn_in(self.conv_in(x))); x = self.res_blocks(x)\n        p = F.relu(self.policy_bn(self.policy_conv(x))); p = p.view(-1, 2*8*8); policy_logits = self.policy_fc(p)\n        v = F.relu(self.value_bn(self.value_conv(x))); v = v.view(-1, 1*8*8); v = F.relu(self.value_fc1(v)); value = torch.tanh(self.value_fc2(v))\n        return policy_logits, value\n\n# Load the trained model\nMODEL_PATH = '/kaggle/input/othello_model_final.pth/pytorch/default/1/othello_model_final.pth'  # Adjust path if needed (e.g., './othello_model_final.pth')\nmodel = OthelloNetV3().to(DEVICE)\nmodel.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\nmodel.eval()\nprint(\"Model loaded successfully.\")\n\n# Board to tensor (from utils)\ndef board_to_tensor(board: np.ndarray, player: int) -> torch.Tensor:\n    own = (board == player).astype(np.float32)\n    opp = (board == -player).astype(np.float32)\n    return torch.from_numpy(np.stack([own, opp])).unsqueeze(0)\n\n# NN Agent: Selects move based on policy logits, masked by legal moves\ndef nn_move(state: Othello) -> tuple[int, int] | None:\n    player = state.current_player\n    moves = state.get_legal_moves(player)\n    if not moves:\n        return None\n    tensor = board_to_tensor(state.board, player).to(DEVICE)\n    with torch.no_grad():\n        policy_logits, _ = model(tensor)\n    logits = policy_logits[0].cpu().numpy()\n    masked_logits = np.full(64, -np.inf)\n    for r, c in moves:\n        idx = r * 8 + c\n        masked_logits[idx] = logits[idx]\n    best_idx = np.argmax(masked_logits)\n    return divmod(best_idx, 8)\n\n# =============================================================================\n# MINIMAX AGENT WITH ALPHA-BETA PRUNING (Depth 4)\n# =============================================================================\n# Evaluation function: Normalized piece difference from current player's perspective\ndef evaluate(state: Othello) -> float:\n    return np.sum(state.board) * state.current_player\n\n# Alpha-beta search (adapted for Othello class)\ndef alphabeta(state: Othello, depth: int, alpha: float, beta: float) -> tuple[float, tuple[int, int] | None]:\n    player = state.current_player\n    if depth == 0 or state.is_game_over():\n        if state.is_game_over():\n            winner = state.get_winner()\n            if winner == player:\n                return float('inf'), None\n            elif winner == 0:\n                return 0, None\n            else:\n                return float('-inf'), None\n        return evaluate(state), None\n    \n    moves = state.get_legal_moves(player)\n    if not moves:\n        # Pass move\n        pass_state = Othello(np.copy(state.board), -player)\n        val, _ = alphabeta(pass_state, depth - 1, -beta, -alpha)\n        return -val, None\n    \n    best_val = float('-inf')\n    best_move = moves[0]\n    for move in moves:\n        new_state = copy.deepcopy(state).make_move(*move, player)\n        val, _ = alphabeta(new_state, depth - 1, -beta, -alpha)\n        val = -val\n        if val > best_val:\n            best_val = val\n            best_move = move\n        alpha = max(alpha, val)\n        if alpha >= beta:\n            break\n    return best_val, best_move\n\n# Minimax move function (depth 4)\ndef minimax_move(state: Othello) -> tuple[int, int] | None:\n    moves = state.get_legal_moves(state.current_player)\n    if not moves:\n        return None\n    _, move = alphabeta(copy.deepcopy(state), 4, float('-inf'), float('inf'))\n    return move\n\n# =============================================================================\n# GAME PLAY AND EVALUATION\n# =============================================================================\n# Play a single game between two agents\ndef play_game(agent1, agent2, verbose=False) -> int:\n    # agent1: function for player 1 (Black), agent2 for -1 (White)\n    game = Othello()\n    agents = {1: agent1, -1: agent2}\n    move_count = 0\n    while not game.is_game_over():\n        agent = agents[game.current_player]\n        move = agent(copy.deepcopy(game))  # Pass copy to avoid mutation issues\n        if move:\n            game.make_move(*move, game.current_player)\n        else:\n            game.current_player = -game.current_player  # Pass\n        move_count += 1\n        if verbose:\n            print(f\"Move {move_count}: Player {game.current_player * -1} played {move}\")\n    winner = game.get_winner()\n    if verbose:\n        print(f\"Game over. Winner: {winner} (1: Black, -1: White, 0: Draw)\")\n    return winner\n\n# Run evaluation: 50 games NN as Black, 50 as White\ndef evaluate_models(num_games_per_side=50):\n    print(f\"Starting evaluation: {num_games_per_side * 2} games total.\")\n    start_time = time.time()\n    \n    # Track results from NN's perspective\n    nn_wins, nn_draws, nn_losses = 0, 0, 0\n    results_log = []\n    \n    # NN as Black (1), Minimax as White (-1)\n    print(\"Evaluating NN as Black...\")\n    for i in tqdm(range(num_games_per_side), desc=\"NN as Black\"):\n        winner = play_game(nn_move, minimax_move)\n        results_log.append(f\"Game {i+1} (NN Black): Winner {winner}\")\n        if winner == 1:\n            nn_wins += 1\n        elif winner == 0:\n            nn_draws += 1\n        else:\n            nn_losses += 1\n    \n    # NN as White (-1), Minimax as Black (1)\n    print(\"Evaluating NN as White...\")\n    for i in tqdm(range(num_games_per_side), desc=\"NN as White\"):\n        winner = play_game(minimax_move, nn_move)\n        results_log.append(f\"Game {i+1 + num_games_per_side} (NN White): Winner {winner}\")\n        if winner == -1:\n            nn_wins += 1\n        elif winner == 0:\n            nn_draws += 1\n        else:\n            nn_losses += 1\n    \n    total_games = num_games_per_side * 2\n    win_rate = (nn_wins / total_games) * 100\n    draw_rate = (nn_draws / total_games) * 100\n    loss_rate = (nn_losses / total_games) * 100\n    \n    summary = f\"\"\"\nEvaluation Summary:\n- NN Wins: {nn_wins} ({win_rate:.2f}%)\n- Draws: {nn_draws} ({draw_rate:.2f}%)\n- NN Losses: {nn_losses} ({loss_rate:.2f}%)\nTotal Time: {(time.time() - start_time) / 60:.2f} minutes\n\"\"\"\n    print(summary)\n    \n    # Save results to file\n    with open('evaluation_results.txt', 'w') as f:\n        f.write(summary + \"\\nDetailed Results:\\n\" + \"\\n\".join(results_log))\n    print(\"Results saved to 'evaluation_results.txt'.\")\n\n# Run the evaluation\nevaluate_models()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}