{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":585084,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":437141,"modelId":453853}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time\nimport random\nimport copy\nimport multiprocessing as mp\nfrom multiprocessing import Pool\nfrom typing import Tuple, List\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\n# =============================================================================\n# 1. SETUP AND DEPENDENCIES (Game Logic, Model Architecture, Minimax AI)\n# =============================================================================\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass Othello:\n    # (Full Othello class is included here for completeness)\n    def __init__(self):\n        self.board = [['.' for _ in range(8)] for _ in range(8)]\n        self.board[3][3], self.board[4][4] = 'W', 'W'\n        self.board[3][4], self.board[4][3] = 'B', 'B'\n        self.current_player = 'B'\n\n    def opponent(self, player: str) -> str:\n        return 'W' if player == 'B' else 'B'\n\n    def get_legal_moves(self, player: str) -> List[Tuple[int, int]]:\n        moves = []\n        for i in range(8):\n            for j in range(8):\n                if self.board[i][j] == '.' and self._is_valid_move(i, j, player):\n                    moves.append((i, j))\n        return moves\n\n    def _is_valid_move(self, row: int, col: int, player: str) -> bool:\n        if self.board[row][col] != '.': return False\n        directions = [(-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 1), (1, -1), (1, 0), (1, 1)]\n        for dr, dc in directions:\n            r, c = row + dr, col + dc\n            if 0 <= r < 8 and 0 <= c < 8 and self.board[r][c] == self.opponent(player):\n                while 0 <= r < 8 and 0 <= c < 8:\n                    if self.board[r][c] == player: return True\n                    if self.board[r][c] == '.': break\n                    r, c = r + dr, c + dc\n        return False\n\n    def make_move(self, row: int, col: int, player: str):\n        self.board[row][col] = player\n        directions = [(-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 1), (1, -1), (1, 0), (1, 1)]\n        for dr, dc in directions:\n            flips = []\n            r, c = row + dr, col + dc\n            while 0 <= r < 8 and 0 <= c < 8 and self.board[r][c] == self.opponent(player):\n                flips.append((r, c))\n                r, c = r + dr, c + dc\n            if 0 <= r < 8 and 0 <= c < 8 and self.board[r][c] == player:\n                for fr, fc in flips:\n                    self.board[fr][fc] = player\n        self.current_player = self.opponent(player)\n\n    def is_game_over(self) -> bool:\n        return not self.get_legal_moves('B') and not self.get_legal_moves('W')\n        \n    def get_score(self) -> Tuple[int, int]:\n        b_count = sum(row.count('B') for row in self.board)\n        w_count = sum(row.count('W') for row in self.board)\n        return b_count, w_count\n\n    def get_winner(self) -> str:\n        b_count, w_count = self.get_score()\n        if b_count > w_count: return 'B'\n        if w_count > b_count: return 'W'\n        return 'Tie'\n        \n    def heuristic(self, player: str) -> int:\n        own_count = sum(row.count(player) for row in self.board)\n        opp_count = sum(row.count(self.opponent(player)) for row in self.board)\n        disc_score = own_count - opp_count\n        own_mob = len(self.get_legal_moves(player))\n        opp_mob = len(self.get_legal_moves(self.opponent(player)))\n        mob_score = own_mob - opp_mob\n        corners = [(0,0), (0,7), (7,0), (7,7)]\n        corner_score = 0\n        for r, c in corners:\n            if self.board[r][c] == player: corner_score += 50\n            elif self.board[r][c] == self.opponent(player): corner_score -= 50\n        return 10 * disc_score + 20 * mob_score + corner_score\n\nclass ResidualBlock(nn.Module):\n    # (ResidualBlock class definition)\n    def __init__(self, num_channels):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(num_channels)\n        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(num_channels)\n\n    def forward(self, x):\n        identity = x\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += identity\n        return F.relu(out)\n\nclass OthelloNetV3(nn.Module):\n    # (OthelloNetV3 class definition)\n    def __init__(self, num_res_blocks=5):\n        super(OthelloNetV3, self).__init__()\n        self.conv_in = nn.Conv2d(2, 128, kernel_size=3, padding=1)\n        self.bn_in = nn.BatchNorm2d(128)\n        self.res_blocks = nn.Sequential(*[ResidualBlock(128) for _ in range(num_res_blocks)])\n        self.policy_conv = nn.Conv2d(128, 2, kernel_size=1)\n        self.policy_bn = nn.BatchNorm2d(2)\n        self.policy_fc = nn.Linear(2 * 8 * 8, 64)\n        self.value_conv = nn.Conv2d(128, 1, kernel_size=1)\n        self.value_bn = nn.BatchNorm2d(1)\n        self.value_fc1 = nn.Linear(1 * 8 * 8, 256)\n        self.value_fc2 = nn.Linear(256, 1)\n\n    def forward(self, x):\n        x = F.relu(self.bn_in(self.conv_in(x)))\n        x = self.res_blocks(x)\n        p = F.relu(self.policy_bn(self.policy_conv(x)))\n        p = p.view(-1, 2 * 8 * 8)\n        policy_logits = self.policy_fc(p)\n        v = F.relu(self.value_bn(self.value_conv(x)))\n        v = v.view(-1, 1 * 8 * 8)\n        v = F.relu(self.value_fc1(v))\n        value = torch.tanh(self.value_fc2(v))\n        return policy_logits, value\n        \ndef minimax(game: Othello, depth: int, alpha: float, beta: float, maximizing: bool, player: str) -> int:\n    # (Minimax function definition)\n    if depth == 0 or game.is_game_over():\n        return game.heuristic(player)\n    legal_moves = game.get_legal_moves(game.current_player)\n    if not legal_moves:\n        return game.heuristic(player)\n    if maximizing:\n        max_eval = -np.inf\n        for move in legal_moves:\n            new_game = copy.deepcopy(game)\n            new_game.make_move(*move, new_game.current_player)\n            eval = minimax(new_game, depth - 1, alpha, beta, False, player)\n            max_eval = max(max_eval, eval)\n            alpha = max(alpha, eval)\n            if beta <= alpha: break\n        return max_eval\n    else:\n        min_eval = np.inf\n        for move in legal_moves:\n            new_game = copy.deepcopy(game)\n            new_game.make_move(*move, new_game.current_player)\n            eval = minimax(new_game, depth - 1, alpha, beta, True, player)\n            min_eval = min(min_eval, eval)\n            beta = min(beta, eval)\n            if beta <= alpha: break\n        return min_eval\n\ndef get_best_move(game: Othello, depth: int) -> Tuple[int, int]:\n    # (get_best_move for Minimax)\n    player = game.current_player\n    legal_moves = game.get_legal_moves(player)\n    if not legal_moves: return None\n    best_move = legal_moves[0]\n    best_val = -np.inf\n    for move in legal_moves:\n        new_game = copy.deepcopy(game)\n        new_game.make_move(*move, player)\n        val = minimax(new_game, depth - 1, -np.inf, np.inf, False, player)\n        if val > best_val:\n            best_val = val\n            best_move = move\n    return best_move\n\n# =============================================================================\n# 2. HELPER FUNCTIONS FOR GAMEPLAY\n# =============================================================================\n# (All helper functions from previous script are included here)\ndef print_board(game: Othello):\n    board = game.board\n    print(\"\\n   A B C D E F G H\")\n    for i in range(8):\n        print(f\"{i+1}  \" + \" \".join(board[i]))\n    b_score, w_score = game.get_score()\n    print(f\"\\nScore: Black (B) {b_score} - White (W) {w_score}\")\n    print(f\"Current Player: {game.current_player}\\n\")\n\ndef board_to_tensor(board: List[List[str]], player: str) -> torch.Tensor:\n    own = np.zeros((8, 8), dtype=np.float32)\n    opp = np.zeros((8, 8), dtype=np.float32)\n    for i in range(8):\n        for j in range(8):\n            if board[i][j] == player:\n                own[i, j] = 1.0\n            elif board[i][j] != '.':\n                opp[i, j] = 1.0\n    return torch.from_numpy(np.stack([own, opp])).unsqueeze(0)\n\ndef get_cnn_move(model: nn.Module, game: Othello, verbose=False) -> Tuple[int, int]:\n    legal_moves = game.get_legal_moves(game.current_player)\n    if not legal_moves: return None\n    with torch.no_grad():\n        tensor = board_to_tensor(game.board, game.current_player).to(DEVICE)\n        policy_logits, value = model(tensor)\n        logits = policy_logits.squeeze(0).cpu().numpy()\n        if verbose:\n            print(f\"AI Estimated Win Chance: {value.item()*100:.1f}%\")\n        legal_indices = [r * 8 + c for r, c in legal_moves]\n        masked_logits = np.full(64, -np.inf)\n        masked_logits[legal_indices] = logits[legal_indices]\n        move_idx = np.argmax(masked_logits)\n        return divmod(move_idx, 8)\n\ndef get_human_move(game: Othello) -> Tuple[int, int]:\n    legal_moves = game.get_legal_moves(game.current_player)\n    if not legal_moves: return None\n    while True:\n        move_str = input(\"Enter your move (e.g., 'C4'): \").strip().upper()\n        if len(move_str) == 2 and 'A' <= move_str[0] <= 'H' and '1' <= move_str[1] <= '8':\n            col = ord(move_str[0]) - ord('A')\n            row = int(move_str[1]) - 1\n            if (row, col) in legal_moves:\n                return (row, col)\n            else:\n                print(\"Invalid move. That square is not a legal move.\")\n        else:\n            print(\"Invalid format. Please use format like 'C4'.\")\n\n# =============================================================================\n# 3. GAMEPLAY AND EVALUATION FUNCTIONS\n# =============================================================================\ndef play_vs_agent(model: nn.Module, human_player: str):\n    # (This function remains unchanged)\n    game = Othello()\n    while not game.is_game_over():\n        print_board(game)\n        if not game.get_legal_moves(game.current_player):\n            print(f\"Player {game.current_player} has no moves and must pass.\")\n            game.current_player = game.opponent(game.current_player)\n            continue\n        if game.current_player == human_player:\n            move = get_human_move(game)\n        else:\n            print(\"AI is thinking...\")\n            move = get_cnn_move(model, game, verbose=True)\n        if move:\n            game.make_move(*move, game.current_player)\n    print(\"\\n================ GAME OVER ================\")\n    print_board(game)\n    print(f\"Winner: {game.get_winner()}\")\n    print(\"=========================================\\n\")\n    \ndef play_agent_vs_minimax(model: nn.Module, cnn_player: str, minimax_depth: int) -> float:\n    \"\"\"Plays a single game between the CNN and Minimax, returns score for CNN.\"\"\"\n    game = Othello()\n    while not game.is_game_over():\n        current_player = game.current_player\n        if not game.get_legal_moves(current_player):\n            game.current_player = game.opponent(current_player)\n            continue\n        \n        if current_player == cnn_player:\n            move = get_cnn_move(model, game)\n        else: # Minimax's turn\n            move = get_best_move(game, minimax_depth)\n        \n        if move:\n            game.make_move(*move, current_player)\n\n    winner = game.get_winner()\n    if winner == cnn_player: return 1.0  # CNN Win\n    if winner == 'Tie': return 0.5       # Tie\n    return 0.0                           # CNN Loss\n\ndef play_single_game_worker(args):\n    \"\"\"Wrapper function for multiprocessing. Loads model inside the process.\"\"\"\n    game_idx, model_path, minimax_depth = args\n    \n    # Each process loads its own copy of the model\n    local_model = OthelloNetV3().to(DEVICE)\n    local_model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n    local_model.eval()\n    \n    # Alternate starting player for fairness\n    cnn_player = 'B' if game_idx % 2 == 0 else 'W'\n    \n    return play_agent_vs_minimax(local_model, cnn_player, minimax_depth)\n\ndef run_batch_evaluation(model_path: str, num_games: int, opponent_depth: int):\n    \"\"\"Runs a batch of games in parallel and prints the results.\"\"\"\n    print(f\"\\n--- Starting Batch Evaluation ---\")\n    print(f\"Opponent: Minimax (Depth {opponent_depth})\")\n    print(f\"Total Games: {num_games}\")\n    print(f\"Using {mp.cpu_count()} CPU cores for parallel games.\")\n    \n    start_time = time.time()\n    \n    with Pool(mp.cpu_count()) as p:\n        args = [(i, model_path, opponent_depth) for i in range(num_games)]\n        results = list(tqdm(p.imap(play_single_game_worker, args), total=num_games, desc=\"Running Games\"))\n    \n    cnn_wins = results.count(1.0)\n    ties = results.count(0.5)\n    cnn_losses = results.count(0.0)\n    \n    end_time = time.time()\n    \n    print(\"\\n--- Evaluation Results ---\")\n    print(\"+------------------+-------+\")\n    print(\"| Metric           | Value |\")\n    print(\"+------------------+-------+\")\n    print(f\"| CNN Wins         | {cnn_wins:<5} |\")\n    print(f\"| Minimax Wins     | {cnn_losses:<5} |\")\n    print(f\"| Ties             | {ties:<5} |\")\n    print(\"+------------------+-------+\")\n    win_rate = (cnn_wins + 0.5 * ties) / num_games * 100\n    print(f\"| CNN Win Rate     | {win_rate:.2f}% |\")\n    print(\"+------------------+-------+\")\n    print(f\"Total time taken: {end_time - start_time:.2f} seconds.\")\n    print(\"----------------------------\\n\")\n\n# =============================================================================\n# 4. MAIN EXECUTION BLOCK\n# =============================================================================\nif __name__ == \"__main__\":\n    MODEL_SAVE_PATH = 'othello_model_v3.pth'\n    \n    try:\n        mp.set_start_method('spawn')\n    except RuntimeError:\n        pass\n    \n    print(\"--- Othello AI Evaluation Suite ---\")\n    print(f\"Using device: {DEVICE}\")\n\n    print(f\"Loading model from {MODEL_SAVE_PATH}...\")\n    try:\n        model = OthelloNetV3().to(DEVICE)\n        model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n        model.eval()\n        print(\"Model loaded successfully!\")\n    except Exception as e:\n        print(f\"ERROR loading model: {e}\")\n        exit()\n\n    while True:\n        print(\"\\nChoose an option:\")\n        print(\"1. Play a game against the AI\")\n        print(\"2. Run 100-game evaluation vs. Minimax Depth 5\")\n        print(\"3. Exit\")\n        choice = input(\"Enter your choice (1/2/3): \").strip()\n\n        if choice == '1':\n            color = input(\"Play as Black (B) or White (W)?: \").strip().upper()\n            if color in ['B', 'W']:\n                play_vs_agent(model, color)\n            else:\n                print(\"Invalid choice.\")\n        elif choice == '2':\n            run_batch_evaluation(\n                model_path=MODEL_SAVE_PATH,\n                num_games=100,\n                opponent_depth=5\n            )\n        elif choice == '3':\n            print(\"Exiting.\")\n            break\n        else:\n            print(\"Invalid choice. Please enter 1, 2, or 3.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}