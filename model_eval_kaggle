{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc0606c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-19T21:01:16.137354Z",
     "iopub.status.busy": "2025-09-19T21:01:16.137163Z",
     "iopub.status.idle": "2025-09-19T21:01:23.536755Z",
     "shell.execute_reply": "2025-09-19T21:01:23.535999Z"
    },
    "papermill": {
     "duration": 7.403337,
     "end_time": "2025-09-19T21:01:23.537852",
     "exception": false,
     "start_time": "2025-09-19T21:01:16.134515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Othello AI Evaluation ---\n",
      "\n",
      "--- Starting Evaluation ---\n",
      "No multiprocessing. It's not like I couldn't handle it, but your environment is just too clumsy.\n",
      "\n",
      "Ugh, something broke. It's probably your fault. Error: [Errno 21] Is a directory: '/kaggle/input/othello_model_v3.pth/pytorch/default/1'\n"
     ]
    }
   ],
   "source": [
    "# run_evaluation_sequential.py\n",
    "\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================================================================\n",
    "# 1. SETUP AND DEPENDENCIES \n",
    "# (You need all of this just to get things working. Don't mess with it.)\n",
    "# =============================================================================\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# The game board. Obviously.\n",
    "class Othello:\n",
    "    def __init__(self):\n",
    "        self.board = [['.' for _ in range(8)] for _ in range(8)]\n",
    "        self.board[3][3], self.board[4][4] = 'W', 'W'\n",
    "        self.board[3][4], self.board[4][3] = 'B', 'B'\n",
    "        self.current_player = 'B'\n",
    "\n",
    "    def opponent(self, player: str) -> str:\n",
    "        return 'W' if player == 'B' else 'B'\n",
    "\n",
    "    def get_legal_moves(self, player: str) -> List[Tuple[int, int]]:\n",
    "        moves = []\n",
    "        for i in range(8):\n",
    "            for j in range(8):\n",
    "                if self.board[i][j] == '.' and self._is_valid_move(i, j, player):\n",
    "                    moves.append((i, j))\n",
    "        return moves\n",
    "\n",
    "    def _is_valid_move(self, row: int, col: int, player: str) -> bool:\n",
    "        if self.board[row][col] != '.': return False\n",
    "        directions = [(-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 1), (1, -1), (1, 0), (1, 1)]\n",
    "        for dr, dc in directions:\n",
    "            r, c = row + dr, col + dc\n",
    "            if 0 <= r < 8 and 0 <= c < 8 and self.board[r][c] == self.opponent(player):\n",
    "                while 0 <= r < 8 and 0 <= c < 8:\n",
    "                    if self.board[r][c] == player: return True\n",
    "                    if self.board[r][c] == '.': break\n",
    "                    r, c = r + dr, c + dc\n",
    "        return False\n",
    "\n",
    "    def make_move(self, row: int, col: int, player: str):\n",
    "        self.board[row][col] = player\n",
    "        directions = [(-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 1), (1, -1), (1, 0), (1, 1)]\n",
    "        for dr, dc in directions:\n",
    "            flips = []\n",
    "            r, c = row + dr, col + dc\n",
    "            while 0 <= r < 8 and 0 <= c < 8 and self.board[r][c] == self.opponent(player):\n",
    "                flips.append((r, c))\n",
    "                r, c = r + dr, c + dc\n",
    "            if 0 <= r < 8 and 0 <= c < 8 and self.board[r][c] == player:\n",
    "                for fr, fc in flips:\n",
    "                    self.board[fr][fc] = player\n",
    "        self.current_player = self.opponent(player)\n",
    "\n",
    "    def is_game_over(self) -> bool:\n",
    "        return not self.get_legal_moves('B') and not self.get_legal_moves('W')\n",
    "        \n",
    "    def get_score(self) -> Tuple[int, int]:\n",
    "        b_count = sum(row.count('B') for row in self.board)\n",
    "        w_count = sum(row.count('W') for row in self.board)\n",
    "        return b_count, w_count\n",
    "\n",
    "    def get_winner(self) -> str:\n",
    "        b_count, w_count = self.get_score()\n",
    "        if b_count > w_count: return 'B'\n",
    "        if w_count > b_count: return 'W'\n",
    "        return 'Tie'\n",
    "        \n",
    "    def heuristic(self, player: str) -> int:\n",
    "        own_count = sum(row.count(player) for row in self.board)\n",
    "        opp_count = sum(row.count(self.opponent(player)) for row in self.board)\n",
    "        disc_score = own_count - opp_count\n",
    "        own_mob = len(self.get_legal_moves(player))\n",
    "        opp_mob = len(self.get_legal_moves(self.opponent(player)))\n",
    "        mob_score = own_mob - opp_mob\n",
    "        corners = [(0,0), (0,7), (7,0), (7,7)]\n",
    "        corner_score = 0\n",
    "        for r, c in corners:\n",
    "            if self.board[r][c] == player: corner_score += 50\n",
    "            elif self.board[r][c] == self.opponent(player): corner_score -= 50\n",
    "        return 10 * disc_score + 20 * mob_score + corner_score\n",
    "\n",
    "# My brilliant architecture. Obviously.\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += identity\n",
    "        return F.relu(out)\n",
    "\n",
    "class OthelloNetV3(nn.Module):\n",
    "    def __init__(self, num_res_blocks=5):\n",
    "        super(OthelloNetV3, self).__init__()\n",
    "        self.conv_in = nn.Conv2d(2, 128, kernel_size=3, padding=1)\n",
    "        self.bn_in = nn.BatchNorm2d(128)\n",
    "        self.res_blocks = nn.Sequential(*[ResidualBlock(128) for _ in range(num_res_blocks)])\n",
    "        self.policy_conv = nn.Conv2d(128, 2, kernel_size=1)\n",
    "        self.policy_bn = nn.BatchNorm2d(2)\n",
    "        self.policy_fc = nn.Linear(2 * 8 * 8, 64)\n",
    "        self.value_conv = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        self.value_bn = nn.BatchNorm2d(1)\n",
    "        self.value_fc1 = nn.Linear(1 * 8 * 8, 256)\n",
    "        self.value_fc2 = nn.Linear(256, 1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn_in(self.conv_in(x)))\n",
    "        x = self.res_blocks(x)\n",
    "        p = F.relu(self.policy_bn(self.policy_conv(x)))\n",
    "        p = p.view(-1, 2 * 8 * 8)\n",
    "        policy_logits = self.policy_fc(p)\n",
    "        v = F.relu(self.value_bn(self.value_conv(x)))\n",
    "        v = v.view(-1, 1 * 8 * 8)\n",
    "        v = F.relu(self.value_fc1(v))\n",
    "        value = torch.tanh(self.value_fc2(v))\n",
    "        return policy_logits, value\n",
    "\n",
    "# The slow, dumb Minimax thing you're testing against.\n",
    "def minimax(game: Othello, depth: int, alpha: float, beta: float, maximizing: bool, player: str) -> int:\n",
    "    if depth == 0 or game.is_game_over(): return game.heuristic(player)\n",
    "    legal_moves = game.get_legal_moves(game.current_player)\n",
    "    if not legal_moves: return game.heuristic(player)\n",
    "    if maximizing:\n",
    "        max_eval = -np.inf\n",
    "        for move in legal_moves:\n",
    "            new_game = copy.deepcopy(game)\n",
    "            new_game.make_move(*move, new_game.current_player)\n",
    "            eval = minimax(new_game, depth - 1, alpha, beta, False, player)\n",
    "            max_eval = max(max_eval, eval)\n",
    "            alpha = max(alpha, eval)\n",
    "            if beta <= alpha: break\n",
    "        return max_eval\n",
    "    else:\n",
    "        min_eval = np.inf\n",
    "        for move in legal_moves:\n",
    "            new_game = copy.deepcopy(game)\n",
    "            new_game.make_move(*move, new_game.current_player)\n",
    "            eval = minimax(new_game, depth - 1, alpha, beta, True, player)\n",
    "            min_eval = min(min_eval, eval)\n",
    "            beta = min(beta, eval)\n",
    "            if beta <= alpha: break\n",
    "        return min_eval\n",
    "\n",
    "def get_best_move(game: Othello, depth: int) -> Tuple[int, int]:\n",
    "    player = game.current_player\n",
    "    legal_moves = game.get_legal_moves(player)\n",
    "    if not legal_moves: return None\n",
    "    best_move = random.choice(legal_moves)\n",
    "    best_val = -np.inf\n",
    "    for move in legal_moves:\n",
    "        new_game = copy.deepcopy(game)\n",
    "        new_game.make_move(*move, player)\n",
    "        val = minimax(new_game, depth - 1, -np.inf, np.inf, False, player)\n",
    "        if val > best_val:\n",
    "            best_val = val\n",
    "            best_move = move\n",
    "    return best_move\n",
    "\n",
    "# =============================================================================\n",
    "# 2. HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "def board_to_tensor(board: List[List[str]], player: str) -> torch.Tensor:\n",
    "    own = np.zeros((8, 8), dtype=np.float32)\n",
    "    opp = np.zeros((8, 8), dtype=np.float32)\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            if board[i][j] == player: own[i, j] = 1.0\n",
    "            elif board[i][j] != '.': opp[i, j] = 1.0\n",
    "    return torch.from_numpy(np.stack([own, opp])).unsqueeze(0)\n",
    "\n",
    "def get_cnn_move(model: nn.Module, game: Othello) -> Tuple[int, int]:\n",
    "    legal_moves = game.get_legal_moves(game.current_player)\n",
    "    if not legal_moves: return None\n",
    "    with torch.no_grad():\n",
    "        tensor = board_to_tensor(game.board, game.current_player).to(DEVICE)\n",
    "        policy_logits, _ = model(tensor)\n",
    "        logits = policy_logits.squeeze(0).cpu().numpy()\n",
    "        legal_indices = [r * 8 + c for r, c in legal_moves]\n",
    "        masked_logits = np.full(64, -np.inf)\n",
    "        masked_logits[legal_indices] = logits[legal_indices]\n",
    "        move_idx = np.argmax(masked_logits)\n",
    "        return divmod(move_idx, 8)\n",
    "\n",
    "def play_agent_vs_minimax(model: nn.Module, cnn_player: str, minimax_depth: int) -> float:\n",
    "    game = Othello()\n",
    "    while not game.is_game_over():\n",
    "        current_player = game.current_player\n",
    "        if not game.get_legal_moves(current_player):\n",
    "            game.current_player = game.opponent(current_player)\n",
    "            continue\n",
    "        if current_player == cnn_player:\n",
    "            move = get_cnn_move(model, game)\n",
    "        else:\n",
    "            move = get_best_move(game, minimax_depth)\n",
    "        if move:\n",
    "            game.make_move(*move, current_player)\n",
    "    winner = game.get_winner()\n",
    "    if winner == cnn_player: return 1.0\n",
    "    if winner == 'Tie': return 0.5\n",
    "    return 0.0\n",
    "\n",
    "# =============================================================================\n",
    "# 3. MAIN BATCH EVALUATION FUNCTION\n",
    "# =============================================================================\n",
    "def run_sequential_evaluation(model_path: str, num_games: int, opponent_depth: int):\n",
    "    print(f\"\\n--- Starting Evaluation ---\")\n",
    "    print(\"No multiprocessing. It's not like I couldn't handle it, but your environment is just too clumsy.\")\n",
    "    \n",
    "    model = OthelloNetV3().to(DEVICE)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "    model.eval()\n",
    "\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Here's  the  stupid loop. I made it work, so don't complain.\n",
    "    for i in tqdm(range(num_games), desc=\"Running games... whatever\"):\n",
    "        cnn_player = 'B' if i % 2 == 0 else 'W'\n",
    "        score = play_agent_vs_minimax(model, cnn_player, opponent_depth)\n",
    "        results.append(score)\n",
    "\n",
    "    cnn_wins = results.count(1.0)\n",
    "    ties = results.count(0.5)\n",
    "    cnn_losses = results.count(0.0)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(\"\\n--- Evaluation Results ---\")\n",
    "    print(\"+------------------+-------+\")\n",
    "    print(\"| Metric           | Value |\")\n",
    "    print(\"+------------------+-------+\")\n",
    "    print(f\"| My Wins          | {cnn_wins:<5} |\") # It's not \"CNN Wins\". It's MY win.\n",
    "    print(f\"| Minimax Wins     | {cnn_losses:<5} |\")\n",
    "    print(f\"| Ties             | {ties:<5} |\")\n",
    "    print(\"+------------------+-------+\")\n",
    "    win_rate = (cnn_wins + 0.5 * ties) / num_games * 100\n",
    "    print(f\"| My Win Rate      | {win_rate:5.2f}% |\")\n",
    "    print(\"+------------------+-------+\")\n",
    "    print(f\"Total time taken: {end_time - start_time:.2f} seconds.\")\n",
    "    print(\"----------------------------\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. MAIN EXECUTION BLOCK\n",
    "# It all starts here. \n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    MODEL_SAVE_PATH = '/kaggle/input/othello_model_v3.pth/pytorch/default/1'\n",
    "    NUM_GAMES_TO_EVAL = 100\n",
    "    OPPONENT_DEPTH = 4\n",
    "    \n",
    "    print(f\"--- Othello AI Evaluation ---\")\n",
    "    \n",
    "    try:\n",
    "        run_sequential_evaluation(\n",
    "            model_path=MODEL_SAVE_PATH,\n",
    "            num_games=NUM_GAMES_TO_EVAL,\n",
    "            opponent_depth=OPPONENT_DEPTH\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"\\nUgh, something broke. It's probably your fault. Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 453853,
     "modelInstanceId": 437141,
     "sourceId": 585084,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.663907,
   "end_time": "2025-09-19T21:01:25.159224",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-19T21:01:10.495317",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
