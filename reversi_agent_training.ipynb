{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO3NMvAezXI73lUYrhSijQv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Agrannya-Singh/othello_api/blob/main/reversi_agent_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc0iyrHI6rrs"
      },
      "outputs": [],
      "source": [
        "%%writefile game_logic.py\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Constants\n",
        "BLACK = 1\n",
        "WHITE = -1\n",
        "EMPTY = 0\n",
        "BOARD_SIZE = 8\n",
        "DIRECTIONS = [(-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 1), (1, -1), (1, 0), (1, 1)]\n",
        "\n",
        "class Othello:\n",
        "    \"\"\"Handles all Othello game logic for the ML agent.\"\"\"\n",
        "    def __init__(self, model=None):\n",
        "        self.board = np.zeros((BOARD_SIZE, BOARD_SIZE), dtype=int)\n",
        "        self.board[3][3] = WHITE\n",
        "        self.board[3][4] = BLACK\n",
        "        self.board[4][3] = BLACK\n",
        "        self.board[4][4] = WHITE\n",
        "        self.current_player = BLACK\n",
        "        self.model = model\n",
        "\n",
        "    def make_move(self, move):\n",
        "        if move is None or not self.is_valid_move(move[0], move[1]):\n",
        "            return False\n",
        "        row, col = move\n",
        "        self.board[row][col] = self.current_player\n",
        "        for dr, dc in DIRECTIONS:\n",
        "            r, c = row + dr, col + dc\n",
        "            to_flip = []\n",
        "            while 0 <= r < BOARD_SIZE and 0 <= c < BOARD_SIZE and self.board[r][c] == -self.current_player:\n",
        "                to_flip.append((r, c))\n",
        "                r += dr\n",
        "                c += dc\n",
        "            if 0 <= r < BOARD_SIZE and 0 <= c < BOARD_SIZE and self.board[r][c] == self.current_player:\n",
        "                for fr, fc in to_flip:\n",
        "                    self.board[fr][fc] = self.current_player\n",
        "        return True\n",
        "\n",
        "    def get_valid_moves(self):\n",
        "        valid_moves = []\n",
        "        for r in range(BOARD_SIZE):\n",
        "            for c in range(BOARD_SIZE):\n",
        "                if self.is_valid_move(r, c):\n",
        "                    valid_moves.append((r, c))\n",
        "        return valid_moves\n",
        "\n",
        "    def is_valid_move(self, row, col):\n",
        "        if self.board[row][col] != EMPTY:\n",
        "            return False\n",
        "        for dr, dc in DIRECTIONS:\n",
        "            r, c = row + dr, col + dc\n",
        "            if 0 <= r < BOARD_SIZE and 0 <= c < BOARD_SIZE and self.board[r][c] == -self.current_player:\n",
        "                r += dr\n",
        "                c += dc\n",
        "                while 0 <= r < BOARD_SIZE and 0 <= c < BOARD_SIZE:\n",
        "                    if self.board[r][c] == self.current_player: return True\n",
        "                    if self.board[r][c] == EMPTY: break\n",
        "                    r += dr\n",
        "                    c += dc\n",
        "        return False\n",
        "\n",
        "    def simulate_move(self, board, player, move):\n",
        "        row, col = move\n",
        "        new_board = board.copy()\n",
        "        new_board[row][col] = player\n",
        "        for dr, dc in DIRECTIONS:\n",
        "            r, c = row + dr, col + dc\n",
        "            to_flip = []\n",
        "            while 0 <= r < BOARD_SIZE and 0 <= c < BOARD_SIZE and new_board[r][c] == -player:\n",
        "                to_flip.append((r, c))\n",
        "                r += dr\n",
        "                c += dc\n",
        "            if 0 <= r < BOARD_SIZE and 0 <= c < BOARD_SIZE and new_board[r][c] == player:\n",
        "                for fr, fc in to_flip:\n",
        "                    new_board[fr][fc] = player\n",
        "        return new_board\n",
        "\n",
        "    def get_best_move_ml(self):\n",
        "        valid_moves = self.get_valid_moves()\n",
        "        if not valid_moves: return None\n",
        "        if self.model is None: return random.choice(valid_moves)\n",
        "\n",
        "        temp_boards = []\n",
        "        for move in valid_moves:\n",
        "            temp_board = self.simulate_move(self.board, self.current_player, move)\n",
        "            temp_boards.append(temp_board.flatten())\n",
        "\n",
        "        if temp_boards:\n",
        "            batch = np.vstack(temp_boards)\n",
        "            scores = self.model.predict(batch, verbose=0).flatten()\n",
        "        else:\n",
        "            scores = []\n",
        "\n",
        "        if self.current_player == BLACK:\n",
        "            best_idx = np.argmax(scores)\n",
        "        else:\n",
        "            best_idx = np.argmin(scores)\n",
        "\n",
        "        return valid_moves[best_idx]\n",
        "\n",
        "    def is_game_over(self):\n",
        "        if self.get_valid_moves(): return False\n",
        "        self.current_player *= -1\n",
        "        has_moves = bool(self.get_valid_moves())\n",
        "        self.current_player *= -1\n",
        "        return not has_moves\n",
        "\n",
        "    def get_winner(self):\n",
        "        black_count = np.sum(self.board == BLACK)\n",
        "        white_count = np.sum(self.board == WHITE)\n",
        "        if black_count > white_count: return BLACK\n",
        "        if white_count > black_count: return WHITE\n",
        "        return EMPTY"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating initial model\n",
        "from tensorflow.keras import models, layers, losses\n",
        "\n",
        "BOARD_SIZE = 8\n",
        "MODEL_PATH = 'othello_model.h5'\n",
        "\n",
        "print(\"Creating a new, untrained model blueprint...\")\n",
        "model = models.Sequential([\n",
        "    layers.Dense(128, activation='relu', input_shape=(BOARD_SIZE * BOARD_SIZE,)),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1, activation='tanh')\n",
        "])\n",
        "model.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
        "model.save(MODEL_PATH)\n",
        "print(f\"Success! Initial model saved to '{MODEL_PATH}'.\")"
      ],
      "metadata": {
        "id": "lgFSt7p2642W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train.py\n",
        "\n",
        "from game_logic import Othello, BLACK\n",
        "from tensorflow.keras import models, losses\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "\n",
        "MODEL_PATH = 'othello_model.h5'\n",
        "\n",
        "def run_training_session(num_games=2000, epochs=15):\n",
        "    print(f\"Starting self-play training session for {num_games} games...\")\n",
        "    model = models.load_model(MODEL_PATH)\n",
        "    # Re-compile to ensure compatibility with the Colab environment\n",
        "    model.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
        "\n",
        "    all_game_data = []\n",
        "\n",
        "    for i in range(num_games):\n",
        "        game = Othello(model=model)\n",
        "        game_states = []  # List of (board_vector, current_player)\n",
        "\n",
        "        while not game.is_game_over():\n",
        "            # Store state BEFORE the move\n",
        "            game_states.append((game.board.flatten(), game.current_player))\n",
        "            move = game.get_best_move_ml()\n",
        "            if move is None:\n",
        "                game.current_player *= -1\n",
        "                continue\n",
        "            game.make_move(move)\n",
        "            game.current_player *= -1\n",
        "\n",
        "        outcome = game.get_winner()\n",
        "        for board_vector, player in game_states:\n",
        "            # The label is the final game outcome, from that player's perspective\n",
        "            if outcome == 0:\n",
        "                score = 0.0\n",
        "            else:\n",
        "                score = 1.0 if player == outcome else -1.0\n",
        "            all_game_data.append((board_vector, score))\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"  ...completed game {i+1}/{num_games}\")\n",
        "\n",
        "    X_train = np.array([data[0] for data in all_game_data])\n",
        "    y_train = np.array([data[1] for data in all_game_data])\n",
        "\n",
        "    print(f\"\\nTraining model on data from {num_games} games...\")\n",
        "    model.fit(X_train, y_train, epochs=epochs, batch_size=64, validation_split=0.1)\n",
        "\n",
        "    model.save(MODEL_PATH)\n",
        "    print(f\"Training complete. New model saved to {MODEL_PATH}\")\n",
        "\n",
        "    # Automatically download the model\n",
        "    files.download(MODEL_PATH)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_training_session()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EcGfMFV645x",
        "outputId": "fd7301b6-ced2-427a-be23-b8772b92399e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting self-play training session for 2000 games...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation script for the model:\n",
        "# evaluate.py\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras import models\n",
        "from game_logic import Othello, BLACK, WHITE, EMPTY\n",
        "\n",
        "MODEL_PATH = 'othello_model.h5'\n",
        "\n",
        "def minmax(board, depth, player, alpha, beta):\n",
        "    game = Othello()  # Temp instance for helper functions\n",
        "    game.board = board.copy()\n",
        "    game.current_player = player\n",
        "\n",
        "    if depth == 0 or game.is_game_over():\n",
        "        black_count = np.sum(board == BLACK)\n",
        "        white_count = np.sum(board == WHITE)\n",
        "        return black_count - white_count if player == BLACK else white_count - black_count\n",
        "\n",
        "    valid_moves = game.get_valid_moves()\n",
        "    if not valid_moves:\n",
        "        return minmax(board, depth - 1, -player, alpha, beta)  # Pass turn\n",
        "\n",
        "    if player == BLACK:  # Maximizing\n",
        "        max_eval = -np.inf\n",
        "        for move in valid_moves:\n",
        "            new_board = game.simulate_move(board, player, move)\n",
        "            eval = minmax(new_board, depth - 1, -player, alpha, beta)\n",
        "            max_eval = max(max_eval, eval)\n",
        "            alpha = max(alpha, eval)\n",
        "            if beta <= alpha:\n",
        "                break\n",
        "        return max_eval\n",
        "    else:  # Minimizing\n",
        "        min_eval = np.inf\n",
        "        for move in valid_moves:\n",
        "            new_board = game.simulate_move(board, player, move)\n",
        "            eval = minmax(new_board, depth - 1, -player, alpha, beta)\n",
        "            min_eval = min(min_eval, eval)\n",
        "            beta = min(beta, eval)\n",
        "            if beta <= alpha:\n",
        "                break\n",
        "        return min_eval\n",
        "\n",
        "def get_best_move_minmax(game, depth=5):\n",
        "    valid_moves = game.get_valid_moves()\n",
        "    if not valid_moves: return None\n",
        "\n",
        "    best_move = None\n",
        "    best_value = -np.inf if game.current_player == BLACK else np.inf\n",
        "    alpha = -np.inf\n",
        "    beta = np.inf\n",
        "\n",
        "    for move in valid_moves:\n",
        "        new_board = game.simulate_move(game.board, game.current_player, move)\n",
        "        value = minmax(new_board, depth - 1, -game.current_player, alpha, beta)\n",
        "        if game.current_player == BLACK:\n",
        "            if value > best_value:\n",
        "                best_value = value\n",
        "                best_move = move\n",
        "            alpha = max(alpha, value)\n",
        "        else:\n",
        "            if value < best_value:\n",
        "                best_value = value\n",
        "                best_move = move\n",
        "            beta = min(beta, value)\n",
        "    return best_move\n",
        "\n",
        "def evaluate_model(num_games=20, depth=5):\n",
        "    model = models.load_model(MODEL_PATH)\n",
        "    wins = 0\n",
        "    draws = 0\n",
        "    losses = 0\n",
        "\n",
        "    for i in range(num_games):\n",
        "        # Alternate starting player: even games ML as BLACK, odd as WHITE\n",
        "        ml_player = BLACK if i % 2 == 0 else WHITE\n",
        "        minmax_player = -ml_player\n",
        "\n",
        "        game = Othello(model=model)\n",
        "        game.current_player = BLACK  # Always start with BLACK\n",
        "\n",
        "        while not game.is_game_over():\n",
        "            if game.current_player == ml_player:\n",
        "                move = game.get_best_move_ml()\n",
        "            else:\n",
        "                move = get_best_move_minmax(game, depth=depth)\n",
        "            if move is None:\n",
        "                game.current_player *= -1\n",
        "                continue\n",
        "            game.make_move(move)\n",
        "            game.current_player *= -1\n",
        "\n",
        "        winner = game.get_winner()\n",
        "        if winner == ml_player:\n",
        "            wins += 1\n",
        "        elif winner == minmax_player:\n",
        "            losses += 1\n",
        "        else:\n",
        "            draws += 1\n",
        "\n",
        "        print(f\"Game {i+1}: ML as {ml_player} - Winner: {winner} (Wins: {wins}, Draws: {draws}, Losses: {losses})\")\n",
        "\n",
        "    win_rate = (wins / num_games) * 100\n",
        "    print(f\"\\nFinal Win Rate: {win_rate}% (Wins: {wins}, Draws: {draws}, Losses: {losses})\")\n",
        "    return win_rate >= 80\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    evaluate_model()"
      ],
      "metadata": {
        "id": "0VrhLBpWBWAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8om61f6DBY5W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}