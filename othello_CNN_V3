{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/agrannyasingh/othello-v3?scriptVersionId=262833333\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# =============================================================================\n# 1. IMPORTS AND SETUP\n# =============================================================================\nimport time\nimport random\nimport multiprocessing as mp\nfrom multiprocessing import Pool\nfrom typing import Tuple, List, Optional\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import StepLR\nfrom tqdm import tqdm  # For progress bars\n\n# For mixed precision training (AMP)\nfrom torch.cuda.amp import autocast, GradScaler\n\n# Confirm GPU availability\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"=========================================\")\nprint(f\"Using device: {DEVICE}\")\nprint(f\"CPU Cores: {mp.cpu_count()}\")\nprint(f\"=========================================\\n\")\n\n# =============================================================================\n# 2. OTHELLO GAME LOGIC (Optimized with NumPy and Undo)\n# =============================================================================\nclass Othello:\n    def __init__(self):\n        self.board = np.zeros((8, 8), dtype=int)\n        self.board[3, 3] = self.board[4, 4] = -1  # 'W' = -1\n        self.board[3, 4] = self.board[4, 3] = 1   # 'B' = 1\n        self.current_player = 1  # 'B'\n\n    def opponent(self, player: int) -> int:\n        return -player\n\n    def get_legal_moves(self, player: int) -> List[Tuple[int, int]]:\n        moves = []\n        for i in range(8):\n            for j in range(8):\n                if self.board[i, j] == 0 and self._is_valid_move(i, j, player):\n                    moves.append((i, j))\n        return moves\n\n    def _is_valid_move(self, row: int, col: int, player: int) -> bool:\n        if self.board[row, col] != 0: return False\n        directions = [(-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 1), (1, -1), (1, 0), (1, 1)]\n        for dr, dc in directions:\n            r, c = row + dr, col + dc\n            if 0 <= r < 8 and 0 <= c < 8 and self.board[r, c] == self.opponent(player):\n                while 0 <= r < 8 and 0 <= c < 8:\n                    if self.board[r, c] == player: return True\n                    if self.board[r, c] == 0: break\n                    r, c = r + dr, c + dc\n        return False\n\n    def make_move(self, row: int, col: int, player: int) -> List[Tuple[int, int]]:\n        if self.board[row, col] != 0:\n            raise ValueError(\"Invalid move: position occupied\")\n        self.board[row, col] = player\n        directions = [(-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 1), (1, -1), (1, 0), (1, 1)]\n        all_flips = []\n        for dr, dc in directions:\n            flips = []\n            r, c = row + dr, col + dc\n            while 0 <= r < 8 and 0 <= c < 8 and self.board[r, c] == self.opponent(player):\n                flips.append((r, c))\n                r, c = r + dr, c + dc\n            if 0 <= r < 8 and 0 <= c < 8 and self.board[r, c] == player:\n                for fr, fc in flips:\n                    self.board[fr, fc] = player\n                all_flips.extend(flips)\n        self.current_player = self.opponent(player)\n        return all_flips\n\n    def undo_move(self, row: int, col: int, player: int, flips: List[Tuple[int, int]]):\n        self.board[row, col] = 0\n        opp = self.opponent(player)\n        for fr, fc in flips:\n            self.board[fr, fc] = opp\n        self.current_player = player\n\n    def is_game_over(self) -> bool:\n        return not self.get_legal_moves(1) and not self.get_legal_moves(-1)\n\n    def get_winner(self) -> str:\n        b_count = np.count_nonzero(self.board == 1)\n        w_count = np.count_nonzero(self.board == -1)\n        if b_count > w_count: return 'B'\n        if w_count > b_count: return 'W'\n        return 'Tie'\n\n    def heuristic(self, player: int) -> int:\n        own_count = np.count_nonzero(self.board == player)\n        opp_count = np.count_nonzero(self.board == self.opponent(player))\n        disc_score = own_count - opp_count\n\n        own_mob = len(self.get_legal_moves(player))\n        opp_mob = len(self.get_legal_moves(self.opponent(player)))\n        mob_score = own_mob - opp_mob\n\n        corners = [(0,0), (0,7), (7,0), (7,7)]\n        corner_score = 0\n        for r, c in corners:\n            if self.board[r, c] == player: corner_score += 50\n            elif self.board[r, c] == self.opponent(player): corner_score -= 50\n        \n        return 10 * disc_score + 20 * mob_score + corner_score\n\n# =============================================================================\n# 3. MINIMAX AI LOGIC (Optimized with Undo instead of Copy)\n# =============================================================================\ndef minimax(game: Othello, depth: int, alpha: float, beta: float, maximizing: bool, player: int) -> int:\n    if depth == 0 or game.is_game_over():\n        return game.heuristic(player)\n\n    curr_player = game.current_player\n    legal_moves = game.get_legal_moves(curr_player)\n    if not legal_moves:\n        return game.heuristic(player)\n\n    if maximizing:\n        max_eval = -np.inf\n        for move in legal_moves:\n            flips = game.make_move(*move, curr_player)\n            eval = minimax(game, depth - 1, alpha, beta, False, player)\n            game.undo_move(*move, curr_player, flips)\n            max_eval = max(max_eval, eval)\n            alpha = max(alpha, eval)\n            if beta <= alpha: break\n        return max_eval\n    else:\n        min_eval = np.inf\n        for move in legal_moves:\n            flips = game.make_move(*move, curr_player)\n            eval = minimax(game, depth - 1, alpha, beta, True, player)\n            game.undo_move(*move, curr_player, flips)\n            min_eval = min(min_eval, eval)\n            beta = min(beta, eval)\n            if beta <= alpha: break\n        return min_eval\n\ndef get_best_move(game: Othello, depth: int) -> Optional[Tuple[int, int]]:\n    orig_player = game.current_player\n    legal_moves = game.get_legal_moves(orig_player)\n    if not legal_moves: return None\n    \n    best_move = legal_moves[0]\n    best_val = -np.inf\n    \n    for move in legal_moves:\n        flips = game.make_move(*move, orig_player)\n        val = minimax(game, depth - 1, -np.inf, np.inf, False, orig_player)\n        game.undo_move(*move, orig_player, flips)\n        if val > best_val:\n            best_val = val\n            best_move = move\n    return best_move\n\n# =============================================================================\n# 4. DATA GENERATION & HELPERS\n# =============================================================================\ndef board_to_tensor(board: np.ndarray, player: int) -> torch.Tensor:\n    own = (board == player).astype(np.float32)\n    opp = (board == -player).astype(np.float32)\n    return torch.from_numpy(np.stack([own, opp])).unsqueeze(0)\n\ndef generate_single_sample(args):\n    idx, max_random_moves, teacher_depth = args\n    random.seed(idx)\n    game = Othello()\n    for _ in range(random.randint(10, max_random_moves)):\n        moves = game.get_legal_moves(game.current_player)\n        if not moves: break\n        move = random.choice(moves)\n        flips = game.make_move(*move, game.current_player)\n        # No undo needed here, as we're advancing\n    \n    moves = game.get_legal_moves(game.current_player)\n    if moves:\n        best_move = get_best_move(game, teacher_depth)\n        if best_move:\n            tensor = board_to_tensor(game.board, game.current_player)\n            move_idx = best_move[0] * 8 + best_move[1]\n            return (tensor, move_idx)\n    return None\n\ndef generate_data(num_positions: int, teacher_depth: int):\n    num_cores = mp.cpu_count()\n    print(f\"Generating data using {num_cores} CPU cores...\")\n    with Pool(num_cores) as p:\n        args = [(i, 50, teacher_depth) for i in range(num_positions)]\n        results = list(tqdm(p.imap(generate_single_sample, args), total=num_positions, desc=\"Generating Training Data\"))\n    \n    data = [r for r in results if r is not None]\n    return data\n\n# =============================================================================\n# 5. ALPHA-ZERO INSPIRED NEURAL NETWORK (OthelloNetV3 with more res blocks)\n# =============================================================================\nclass ResidualBlock(nn.Module):\n    \"\"\"A clean, modular Residual Block.\"\"\"\n    def __init__(self, num_channels):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(num_channels)\n        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(num_channels)\n\n    def forward(self, x):\n        identity = x\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += identity\n        return F.relu(out)\n\nclass OthelloNetV3(nn.Module):\n    \"\"\"Dual-head network with a shared body of residual blocks.\"\"\"\n    def __init__(self, num_res_blocks=8):  # Increased to 8 for better capacity\n        super(OthelloNetV3, self).__init__()\n        \n        # --- Convolutional Body ---\n        self.conv_in = nn.Conv2d(2, 128, kernel_size=3, padding=1)\n        self.bn_in = nn.BatchNorm2d(128)\n        self.res_blocks = nn.Sequential(\n            *[ResidualBlock(128) for _ in range(num_res_blocks)]\n        )\n\n        # --- Policy Head ---\n        self.policy_conv = nn.Conv2d(128, 2, kernel_size=1)\n        self.policy_bn = nn.BatchNorm2d(2)\n        self.policy_fc = nn.Linear(2 * 8 * 8, 64)\n\n        # --- Value Head ---\n        self.value_conv = nn.Conv2d(128, 1, kernel_size=1)\n        self.value_bn = nn.BatchNorm2d(1)\n        self.value_fc1 = nn.Linear(1 * 8 * 8, 256)\n        self.value_fc2 = nn.Linear(256, 1)\n\n    def forward(self, x):\n        # --- Body ---\n        x = F.relu(self.bn_in(self.conv_in(x)))\n        x = self.res_blocks(x)\n\n        # --- Policy Head ---\n        p = F.relu(self.policy_bn(self.policy_conv(x)))\n        p = p.view(-1, 2 * 8 * 8)\n        policy_logits = self.policy_fc(p)\n\n        # --- Value Head ---\n        v = F.relu(self.value_bn(self.value_conv(x)))\n        v = v.view(-1, 1 * 8 * 8)\n        v = F.relu(self.value_fc1(v))\n        value = torch.tanh(self.value_fc2(v))\n\n        return policy_logits, value\n\n# =============================================================================\n# 6. TRAINING AND EVALUATION FUNCTIONS (With AMP and Value Head Note)\n# =============================================================================\ndef train_model(data: List[Tuple[torch.Tensor, int]], model_path: str, epochs: int = 25, batch_size: int = 128):\n    print(\"Starting model training...\")\n    model = OthelloNetV3().to(DEVICE)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    scheduler = StepLR(optimizer, step_size=8, gamma=0.1)\n    \n    inputs = torch.cat([d[0] for d in data]).to(DEVICE)\n    targets = torch.tensor([d[1] for d in data], dtype=torch.long).to(DEVICE)\n    dataset = torch.utils.data.TensorDataset(inputs, targets)\n    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    \n    scaler = GradScaler(enabled=(DEVICE.type == 'cuda'))  # For AMP\n    \n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        for batch_inputs, batch_targets in loader:\n            optimizer.zero_grad()\n            with autocast(enabled=(DEVICE.type == 'cuda')):\n                policy_logits, _ = model(batch_inputs)\n                loss = criterion(policy_logits, batch_targets)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            total_loss += loss.item()\n        \n        scheduler.step()\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(loader):.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n    \n    # Note: Value head not trained yet; future: add value targets and loss\n    torch.save(model.state_dict(), model_path)\n    return model\n\ndef play_game(model: nn.Module, opponent_depth: int, model_player: int) -> float:\n    game = Othello()\n    model.eval()\n    \n    while not game.is_game_over():\n        moves = game.get_legal_moves(game.current_player)\n        if not moves:\n            game.current_player = game.opponent(game.current_player)\n            continue\n        \n        if game.current_player == model_player:\n            with torch.no_grad():\n                tensor = board_to_tensor(game.board, game.current_player).to(DEVICE)\n                policy_logits, _ = model(tensor)\n                logits = policy_logits.squeeze(0).cpu().numpy()\n                \n                legal_indices = [r * 8 + c for r, c in moves]\n                masked_logits = np.full(64, -np.inf)\n                masked_logits[legal_indices] = logits[legal_indices]\n                move_idx = np.argmax(masked_logits)\n                move = divmod(move_idx, 8)\n        else:\n            move = get_best_move(game, opponent_depth)\n        \n        if move:\n            _ = game.make_move(*move, game.current_player)  # flips not needed for play\n            \n    winner = game.get_winner()\n    if winner == ('B' if model_player == 1 else 'W'): return 1.0\n    if winner == 'Tie': return 0.5\n    return 0.0\n\ndef play_single_eval_game(args):\n    i, model_path, opponent_depth = args\n    model = OthelloNetV3().to(DEVICE)\n    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n    model_player = 1 if i % 2 == 0 else -1  # 'B' or 'W'\n    return play_game(model, opponent_depth, model_player)\n\ndef evaluate_model(model_path: str, num_games: int, opponent_depth: int):\n    num_cores = mp.cpu_count()\n    print(f\"Evaluating model using {num_cores} CPU cores...\")\n    with Pool(num_cores) as p:\n        args = [(i, model_path, opponent_depth) for i in range(num_games)]\n        scores = list(tqdm(p.imap(play_single_eval_game, args), total=num_games, desc=\"Evaluating Model\"))\n        \n    win_rate = sum(scores) / num_games\n    return win_rate\n\n# =============================================================================\n# 7. MAIN EXECUTION BLOCK\n# =============================================================================\nif __name__ == \"__main__\":\n    # --- Parameters ---\n    NUM_TRAINING_POSITIONS = 10000\n    MINIMAX_TEACHER_DEPTH = 4  # Can increase to 5 with optimizations\n    NUM_EVALUATION_GAMES = 100\n    MINIMAX_OPPONENT_DEPTH = 4\n    MODEL_SAVE_PATH = 'othello_model_v3.pth'\n\n    # --- Phase 1: Data Generation ---\n    start_time = time.time()\n    training_data = generate_data(NUM_TRAINING_POSITIONS, MINIMAX_TEACHER_DEPTH)\n    print(f\"Generated {len(training_data)} samples in {(time.time() - start_time)/60:.2f} minutes.\")\n    \n    # --- Phase 2: Model Training ---\n    trained_model = train_model(training_data, MODEL_SAVE_PATH)\n    print(f\"Model trained and saved to '{MODEL_SAVE_PATH}'\")\n\n    # --- Phase 3: Evaluation ---\n    win_rate = evaluate_model(MODEL_SAVE_PATH, NUM_EVALUATION_GAMES, MINIMAX_OPPONENT_DEPTH)\n    print(f\"\\n========================================================\")\n    print(f\"🚀 Model evaluation complete!\")\n    print(f\"Win rate vs. Minimax Depth {MINIMAX_OPPONENT_DEPTH} over {NUM_EVALUATION_GAMES} games: {win_rate * 100:.2f}%\")\n    print(f\"========================================================\")\n    \n    end_time = time.time()\n    print(f\"Total script runtime: {(end_time - start_time)/60:.2f} minutes.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}