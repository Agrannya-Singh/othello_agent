{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/agrannyasingh/othello-v3?scriptVersionId=262948453\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"69904ef7","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-09-20T09:22:12.834024Z","iopub.status.busy":"2025-09-20T09:22:12.833854Z","iopub.status.idle":"2025-09-20T09:22:12.843419Z","shell.execute_reply":"2025-09-20T09:22:12.84268Z"},"papermill":{"duration":0.013341,"end_time":"2025-09-20T09:22:12.844472","exception":false,"start_time":"2025-09-20T09:22:12.831131","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing othello_utils.py\n"]}],"source":["%%writefile othello_utils.py\n","import math\n","import random\n","import copy\n","import numpy as np\n","import torch\n","\n","# =============================================================================\n","# OTHELLO GAME LOGIC\n","# =============================================================================\n","class Othello:\n","    def __init__(self, board=None, current_player=1):\n","        if board is None:\n","            self.board = np.zeros((8, 8), dtype=np.int8)\n","            self.board[3, 3] = self.board[4, 4] = -1\n","            self.board[3, 4] = self.board[4, 3] = 1\n","        else:\n","            self.board = board\n","        self.current_player = current_player\n","\n","    def opponent(self, player: int) -> int: return -player\n","\n","    def get_legal_moves(self, player: int):\n","        moves = []\n","        for r in range(8):\n","            for c in range(8):\n","                if self.board[r,c] == 0 and self._is_valid_move(r, c, player):\n","                    moves.append((r,c))\n","        return moves\n","\n","    def _is_valid_move(self, row: int, col: int, player: int) -> bool:\n","        if self.board[row, col] != 0: return False\n","        for dr, dc in [(-1,-1),(-1,0),(-1,1),(0,-1),(0,1),(1,-1),(1,0),(1,1)]:\n","            r, c = row + dr, col + dc\n","            if 0 <= r < 8 and 0 <= c < 8 and self.board[r, c] == -player:\n","                r, c = r + dr, c + dc\n","                while 0 <= r < 8 and 0 <= c < 8:\n","                    if self.board[r, c] == player: return True\n","                    if self.board[r, c] == 0: break\n","                    r, c = r + dr, c + dc\n","        return False\n","\n","    def make_move(self, row: int, col: int, player: int):\n","        self.board[row, col] = player\n","        for dr, dc in [(-1,-1),(-1,0),(-1,1),(0,-1),(0,1),(1,-1),(1,0),(1,1)]:\n","            flips = []\n","            r, c = row + dr, col + dc\n","            while 0 <= r < 8 and 0 <= c < 8 and self.board[r, c] == -player:\n","                flips.append((r, c)); r, c = r + dr, c + dc\n","            if 0 <= r < 8 and 0 <= c < 8 and self.board[r, c] == player:\n","                for fr, fc in flips: self.board[fr, fc] = player\n","        self.current_player = -player\n","        return self\n","\n","    def is_game_over(self) -> bool: return not self.get_legal_moves(1) and not self.get_legal_moves(-1)\n","    \n","    def get_winner(self) -> int:\n","        score = np.sum(self.board)\n","        return 1 if score > 0 else -1 if score < 0 else 0\n","\n","# =============================================================================\n","# MCTS IMPLEMENTATION (Corrected)\n","# =============================================================================\n","class MCTSNode:\n","    def __init__(self, state: Othello, parent=None, move=None):\n","        self.state = state\n","        self.parent = parent\n","        self.move = move\n","        self.children = []\n","        self.visits = 0\n","        self.wins = 0.0\n","        self.untried_moves = self.state.get_legal_moves(self.state.current_player)\n","\n","    def ucb1_score(self, exploration_constant=1.41) -> float:\n","        if self.visits == 0: return float('inf')\n","        # The Q-value (wins/visits) must be from the perspective of the parent node.\n","        # Since self.wins is from the current node's perspective, we invert it for the parent.\n","        q_value = 1 - (self.wins / self.visits)\n","        return q_value + exploration_constant * math.sqrt(math.log(self.parent.visits) / self.visits)\n","\n","    def select_best_child(self) -> 'MCTSNode':\n","        return max(self.children, key=lambda c: c.ucb1_score())\n","\n","    def expand(self) -> 'MCTSNode':\n","        move = self.untried_moves.pop()\n","        new_state = Othello(np.copy(self.state.board), self.state.current_player)\n","        new_state.make_move(move[0], move[1], self.state.current_player)\n","        child_node = MCTSNode(new_state, parent=self, move=move)\n","        self.children.append(child_node)\n","        return child_node\n","    \n","    def update(self, outcome: int):\n","        self.visits += 1\n","        # The outcome is absolute (+1 for Black, -1 for White).\n","        # We update 'wins' from the perspective of the player whose turn it is at this node.\n","        # If the player-to-move is the opponent of the winner, their result is a \"win\" (+1).\n","        if self.state.current_player == -outcome:\n","            self.wins += 1.0\n","        # If the player-to-move is the winner, their result is a \"loss\" (-1), as they did not get to capitalize.\n","        # This seems counter-intuitive but correctly represents the value for the *next* player.\n","        # A simpler way is to track value from the parent's perspective. Let's adjust for clarity.\n","        # A value of +1 is a win for the player who made the move to get to this state.\n","        if self.parent.state.current_player == outcome:\n","             self.wins += 1.0\n","\n","\n","def run_mcts(root_state: Othello, num_simulations: int):\n","    root_node = MCTSNode(state=root_state)\n","\n","    if not root_node.untried_moves:\n","        return None # Handle pass turns\n","\n","    for _ in range(num_simulations):\n","        node = root_node\n","        state = copy.deepcopy(root_state)\n","\n","        while not node.untried_moves and node.children:\n","            node = node.select_best_child()\n","            state.make_move(node.move[0], node.move[1], state.current_player)\n","\n","        if node.untried_moves:\n","            node = node.expand()\n","            state.make_move(node.move[0], node.move[1], state.current_player)\n","\n","        while not state.is_game_over():\n","            moves = state.get_legal_moves(state.current_player)\n","            if not moves:\n","                state.current_player = -state.current_player\n","                continue\n","            state.make_move(*random.choice(moves), state.current_player)\n","        \n","        outcome = state.get_winner()\n","        \n","        while node is not None:\n","            node.visits += 1\n","            if node.parent:\n","                if node.parent.state.current_player == outcome:\n","                    node.wins += 1.0\n","            node = node.parent\n","            \n","    return max(root_node.children, key=lambda c: c.visits).move\n","\n","# =============================================================================\n","# DATA GENERATION WORKER\n","# =============================================================================\n","def board_to_tensor(board: np.ndarray, player: int) -> torch.Tensor:\n","    own = (board == player).astype(np.float32)\n","    opp = (board == -player).astype(np.float32)\n","    return torch.from_numpy(np.stack([own, opp])).unsqueeze(0)\n","\n","def generate_single_sample(mcts_sims_per_move: int):\n","    game = Othello()\n","    for _ in range(random.randint(5, 40)):\n","        moves = game.get_legal_moves(game.current_player)\n","        if not moves: break\n","        game.make_move(*random.choice(moves), game.current_player)\n","\n","    if game.is_game_over(): return None\n","    \n","    best_move = run_mcts(copy.deepcopy(game), mcts_sims_per_move)\n","    if best_move is None: return None\n","    \n","    policy_target = best_move[0] * 8 + best_move[1]\n","    \n","    temp_game = copy.deepcopy(game)\n","    while not temp_game.is_game_over():\n","        moves = temp_game.get_legal_moves(temp_game.current_player)\n","        if not moves:\n","            temp_game.current_player = -temp_game.current_player\n","            continue\n","        temp_game.make_move(*random.choice(moves), temp_game.current_player)\n","    \n","    value_target = float(temp_game.get_winner() * game.current_player)\n","    \n","    tensor = board_to_tensor(game.board, game.current_player)\n","    return (tensor, policy_target, value_target)"]},{"cell_type":"code","execution_count":2,"id":"fb5b8b37","metadata":{"execution":{"iopub.execute_input":"2025-09-20T09:22:12.848208Z","iopub.status.busy":"2025-09-20T09:22:12.84801Z","iopub.status.idle":"2025-09-20T11:01:12.753473Z","shell.execute_reply":"2025-09-20T11:01:12.752612Z"},"papermill":{"duration":5939.909006,"end_time":"2025-09-20T11:01:12.755002","exception":false,"start_time":"2025-09-20T09:22:12.845996","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda, CPU Cores: 4\n","\n","Generating 10000 samples using 4 cores...\n"]},{"name":"stderr","output_type":"stream","text":["Generating Training Data: 100%|██████████| 10000/10000 [1:38:12<00:00,  1.70it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Starting model training...\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_19/3683155665.py:75: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler(enabled=(DEVICE.type == 'cuda'))\n","/tmp/ipykernel_19/3683155665.py:80: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast(enabled=(DEVICE.type == 'cuda')):\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/30, Policy Loss: 3.8811, Value Loss: 0.9659\n","Epoch 2/30, Policy Loss: 3.5845, Value Loss: 0.9579\n","Epoch 3/30, Policy Loss: 3.3600, Value Loss: 0.9515\n","Epoch 4/30, Policy Loss: 3.1533, Value Loss: 0.9353\n","Epoch 5/30, Policy Loss: 2.9495, Value Loss: 0.9050\n","Epoch 6/30, Policy Loss: 2.7549, Value Loss: 0.8560\n","Epoch 7/30, Policy Loss: 2.5373, Value Loss: 0.7914\n","Epoch 8/30, Policy Loss: 2.3063, Value Loss: 0.7065\n","Epoch 9/30, Policy Loss: 2.0293, Value Loss: 0.5999\n","Epoch 10/30, Policy Loss: 1.7169, Value Loss: 0.4718\n","Epoch 11/30, Policy Loss: 1.4154, Value Loss: 0.3742\n","Epoch 12/30, Policy Loss: 1.0970, Value Loss: 0.2841\n","Epoch 13/30, Policy Loss: 0.7579, Value Loss: 0.2044\n","Epoch 14/30, Policy Loss: 0.5009, Value Loss: 0.1612\n","Epoch 15/30, Policy Loss: 0.3283, Value Loss: 0.1245\n","Epoch 16/30, Policy Loss: 0.2082, Value Loss: 0.0991\n","Epoch 17/30, Policy Loss: 0.1427, Value Loss: 0.0933\n","Epoch 18/30, Policy Loss: 0.1074, Value Loss: 0.0865\n","Epoch 19/30, Policy Loss: 0.0892, Value Loss: 0.0796\n","Epoch 20/30, Policy Loss: 0.0774, Value Loss: 0.0691\n","Epoch 21/30, Policy Loss: 0.0674, Value Loss: 0.0671\n","Epoch 22/30, Policy Loss: 0.0545, Value Loss: 0.0645\n","Epoch 23/30, Policy Loss: 0.0557, Value Loss: 0.0679\n","Epoch 24/30, Policy Loss: 0.0478, Value Loss: 0.0668\n","Epoch 25/30, Policy Loss: 0.0480, Value Loss: 0.0656\n","Epoch 26/30, Policy Loss: 0.0541, Value Loss: 0.0641\n","Epoch 27/30, Policy Loss: 0.0527, Value Loss: 0.0664\n","Epoch 28/30, Policy Loss: 0.0500, Value Loss: 0.0565\n","Epoch 29/30, Policy Loss: 0.0442, Value Loss: 0.0545\n","Epoch 30/30, Policy Loss: 0.0318, Value Loss: 0.0525\n","\n","Total script runtime: 98.90 minutes.\n"]}],"source":["import time\n","import math\n","import copy\n","import multiprocessing as mp\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.cuda.amp import autocast, GradScaler\n","from tqdm import tqdm\n","import numpy as np\n","\n","# Import from your helper script\n","from othello_utils import generate_single_sample, Othello, board_to_tensor, run_mcts\n","\n","# =============================================================================\n","# SETUP\n","# =============================================================================\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","CPU_COUNT = mp.cpu_count()\n","print(f\"Using device: {DEVICE}, CPU Cores: {CPU_COUNT}\\n\")\n","\n","# =============================================================================\n","# NEURAL NETWORK (BUG FIXED)\n","# =============================================================================\n","class ResidualBlock(nn.Module):\n","    def __init__(self, num_channels):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(num_channels)\n","        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(num_channels)\n","    def forward(self, x):\n","        identity = x; out = F.relu(self.bn1(self.conv1(x))); out = self.bn2(self.conv2(out)); out += identity; return F.relu(out)\n","\n","class OthelloNetV3(nn.Module):\n","    def __init__(self, num_res_blocks=8):\n","        super(OthelloNetV3, self).__init__()\n","        self.conv_in = nn.Conv2d(2, 128, kernel_size=3, padding=1); self.bn_in = nn.BatchNorm2d(128)\n","        self.res_blocks = nn.Sequential(*[ResidualBlock(128) for _ in range(num_res_blocks)])\n","        self.policy_conv = nn.Conv2d(128, 2, kernel_size=1); self.policy_bn = nn.BatchNorm2d(2); self.policy_fc = nn.Linear(2*8*8, 64)\n","        self.value_conv = nn.Conv2d(128, 1, kernel_size=1); self.value_bn = nn.BatchNorm2d(1); self.value_fc1 = nn.Linear(1*8*8, 256); self.value_fc2 = nn.Linear(256, 1)\n","\n","    def forward(self, x):\n","        x = F.relu(self.bn_in(self.conv_in(x))); x = self.res_blocks(x)\n","        \n","        # --- BUG FIX WAS HERE ---\n","        # Changed p.view(--1, ...) to p.view(-1, ...)\n","        p = F.relu(self.policy_bn(self.policy_conv(x))); p = p.view(-1, 2*8*8); policy_logits = self.policy_fc(p)\n","        \n","        v = F.relu(self.value_bn(self.value_conv(x))); v = v.view(-1, 1*8*8); v = F.relu(self.value_fc1(v)); value = torch.tanh(self.value_fc2(v))\n","        return policy_logits, value\n","\n","# =============================================================================\n","# DATA GENERATION & TRAINING (Unchanged)\n","# =============================================================================\n","def generate_data_parallel(num_positions: int, mcts_sims: int):\n","    print(f\"Generating {num_positions} samples using {CPU_COUNT} cores...\")\n","    with mp.Pool(CPU_COUNT) as p:\n","        results = list(tqdm(p.imap(generate_single_sample, [mcts_sims] * num_positions), total=num_positions, desc=\"Generating Training Data\"))\n","    data = [r for r in results if r is not None]\n","    return data\n","\n","def train_model(data, model_path, epochs=30, batch_size=256):\n","    print(\"\\nStarting model training...\")\n","    model = OthelloNetV3().to(DEVICE)\n","    policy_criterion = nn.CrossEntropyLoss(); value_criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    if not data: print(\"No data to train on.\"); return None\n","    inputs = torch.cat([d[0] for d in data]).to(DEVICE)\n","    policy_targets = torch.tensor([d[1] for d in data], dtype=torch.long).to(DEVICE)\n","    value_targets = torch.tensor([d[2] for d in data], dtype=torch.float32).to(DEVICE)\n","    dataset = torch.utils.data.TensorDataset(inputs, policy_targets, value_targets)\n","    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","    scaler = GradScaler(enabled=(DEVICE.type == 'cuda'))\n","    for epoch in range(epochs):\n","        model.train(); total_policy_loss, total_value_loss = 0, 0\n","        for batch_inputs, batch_policy, batch_value in loader:\n","            optimizer.zero_grad()\n","            with autocast(enabled=(DEVICE.type == 'cuda')):\n","                policy_logits, value_preds = model(batch_inputs)\n","                p_loss = policy_criterion(policy_logits, batch_policy)\n","                v_loss = value_criterion(value_preds.squeeze(), batch_value)\n","                total_loss = p_loss + v_loss\n","            scaler.scale(total_loss).backward(); scaler.step(optimizer); scaler.update()\n","            total_policy_loss += p_loss.item(); total_value_loss += v_loss.item()\n","        avg_p_loss = total_policy_loss/len(loader); avg_v_loss = total_value_loss/len(loader)\n","        print(f\"Epoch {epoch+1}/{epochs}, Policy Loss: {avg_p_loss:.4f}, Value Loss: {avg_v_loss:.4f}\")\n","    torch.save(model.state_dict(), model_path); return model\n","\n","# (Evaluation functions are omitted for brevity but should be included in your script)\n","# ...\n","\n","# =============================================================================\n","# MAIN EXECUTION BLOCK (Unchanged)\n","# =============================================================================\n","if __name__ == \"__main__\":\n","    mp.set_start_method('spawn', force=True)\n","    \n","    # Parameters\n","    NUM_TRAINING_POSITIONS = 10000\n","    MCTS_TEACHER_SIMULATIONS = 40\n","    MODEL_SAVE_PATH = 'othello_model_final.pth'\n","    \n","    # Run Pipeline\n","    total_start_time = time.time()\n","    training_data = generate_data_parallel(NUM_TRAINING_POSITIONS, MCTS_TEACHER_SIMULATIONS)\n","    \n","    model = train_model(training_data, MODEL_SAVE_PATH)\n","    \n","    # You can add the evaluation call here if needed.\n","    \n","    print(f\"\\nTotal script runtime: {(time.time() - total_start_time)/60:.2f} minutes.\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":5948.345497,"end_time":"2025-09-20T11:01:15.664595","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-09-20T09:22:07.319098","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}